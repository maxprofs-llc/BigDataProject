{"altmetric_id":4985055,"counts":{"readers":{"mendeley":38,"citeulike":0,"connotea":0},"total":{"posts_count":5},"twitter":{"unique_users_count":5,"unique_users":["RebeccaGelding","michaelhogannui","psych2evidence","mripapers","AV_SP"],"posts_count":5}},"selected_quotes":["Dual temporal mechanisims in human auditory cortex: Evidence from M\/EEG - new","How is the range of perceptually relevant modulation rates encoded in the cerebral cortex? @davidpoeppel dual coding","Dual temporal encoding mechanisms in the human auditory cortex: Evidence ... #Psychiatry","NI: Dual temporal encoding mechanisms in human auditory cortex: Evidence from MEG and EEG"],"citation":{"abstract":"Current hypotheses about language processing advocate an integral relationship between encoding of temporal information and linguistic processing in the brain. All such explanations must accommodate the evident ability of the perceptual system to process both slow and fast time scales in speech. However most cortical neurons are limited in their capability to precisely synchronise to temporal modulations at rates faster than about 50Hz. Hence, a central question in auditory neurophysiology concerns how the full range of perceptually relevant modulation rates might be encoded in the cerebral cortex. Here we show with concurrent noninvasive magnetoencephalography (MEG) and electroencephalography (EEG) measurements that the human auditory cortex transitions between a phase-locked (PL) mode of responding to modulation rates below about 50Hz, and a non-phase-locked (NPL) mode at higher rates. Precisely such dual response modes are predictable from the behaviours of single neurons in auditory cortices of non-human primates. Our data point to a common mechanistic explanation for the single neuron and MEG\/EEG results and support the hypothesis that two distinct types of neuronal encoding mechanisms are employed by the auditory cortex to represent a wide range of temporal modulation rates. This dual encoding model allows slow and fast modulations in speech to be processed in parallel and is therefore consistent with theoretical frameworks in which slow temporal modulations (such as rhythm or syllabic structure) are akin to the contours or edges of visual objects, whereas faster modulations (such as periodicity pitch or phonemic structure) are more like visual texture.","altmetric_jid":"4f6fa50c3cf058f610003196","authors":["Tang, Huizhen","Crain, Stephen","Johnson, Blake W"],"doi":"10.1016\/j.neuroimage.2015.12.053","endpage":"43","first_seen_on":"2016-01-12T23:52:22+00:00","funders":["niehs"],"issns":["10538119","1095-9572"],"journal":"NeuroImage","last_mentioned_on":1452986208,"links":["http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1053811915011660","http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/26763154?dopt=Abstract","http:\/\/www.sciencedirect.com\/science?_ob=GatewayURL&_origin=IRSSSEARCH&_method=citationSearch&_piikey=S1053811915011660&_version=1&md5=7bc832b56186c8b8184e5e905762ad4b","http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/26763154"],"pmid":"26763154","pubdate":"2016-01-15T16:11:01+00:00","publisher_subjects":[{"name":"Medical And Health Sciences","scheme":"era"},{"name":"Psychology And Cognitive Sciences","scheme":"era"}],"scopus_subjects":["Life Sciences","Neuroscience"],"startpage":"32","subjects":["diagnosticimaging"],"title":"Dual temporal encoding mechanisms in the human auditory cortex: Evidence from MEG and EEG.","type":"article","volume":"128","mendeley_url":"http:\/\/www.mendeley.com\/research\/dual-temporal-encoding-mechanisms-human-auditory-cortex-evidence-meg-eeg-2"},"altmetric_score":{"score":2.25,"score_history":{"1y":0,"6m":0,"3m":0,"1m":0,"1w":0,"6d":0,"5d":0,"4d":0,"3d":0,"2d":0,"1d":0,"at":2.25},"context_for_score":{"all":{"total_number_of_other_articles":6978736,"mean":6.2380718316401,"rank":2647067,"this_scored_higher_than_pct":61,"this_scored_higher_than":4286711,"rank_type":"exact","sample_size":6978736,"percentile":61},"similar_age_3m":{"total_number_of_other_articles":304823,"mean":9.1333297071732,"rank":106482,"this_scored_higher_than_pct":64,"this_scored_higher_than":195151,"rank_type":"exact","sample_size":304823,"percentile":64},"this_journal":{"total_number_of_other_articles":4936,"mean":7.380232218845,"rank":2636,"this_scored_higher_than_pct":45,"this_scored_higher_than":2248,"rank_type":"exact","sample_size":4936,"percentile":45},"similar_age_this_journal_3m":{"total_number_of_other_articles":208,"mean":8.0300579710145,"rank":123,"this_scored_higher_than_pct":39,"this_scored_higher_than":82,"rank_type":"exact","sample_size":208,"percentile":39}}},"demographics":{"poster_types":{"member_of_the_public":5},"users":{"twitter":{"cohorts":{"Members of the public":5}},"mendeley":{"by_status":{"Unspecified":1,"Professor > Associate Professor":4,"Student  > Doctoral Student":2,"Researcher":6,"Student  > Ph. D. Student":13,"Student  > Master":6,"Other":1,"Student  > Bachelor":2,"Lecturer":1,"Professor":2},"by_discipline":{"Engineering":7,"Medicine and Dentistry":7,"Neuroscience":10,"Psychology":7,"Agricultural and Biological Sciences":1,"Linguistics":1,"Biochemistry, Genetics and Molecular Biology":1,"Unspecified":4}}},"geo":{"twitter":{"IE":1,"GB":1},"mendeley":{"CA":1,"BE":1,"US":2,"TW":1,"DK":1,"GB":1}}},"posts":{"twitter":[{"url":"http:\/\/twitter.com\/RebeccaGelding\/statuses\/687010286834810880","license":"gnip","citation_ids":[4985055],"posted_on":"2016-01-12T20:36:31+00:00","author":{"name":"Rebecca Gelding","url":"https:\/\/musiconthemindblog.wordpress.com\/","image":"https:\/\/pbs.twimg.com\/profile_images\/378800000577116804\/9ace630cc6c7f44962d9ca8bcea4178e_normal.jpeg","description":"Using MEG to understand what's going on in the brain while people are imagining music; #phdmum #scicomm #musicscience","id_on_source":"RebeccaGelding","tweeter_id":"499636235","geo":{"lt":null,"ln":null},"followers":1286},"tweet_id":"687010286834810880"},{"url":"http:\/\/twitter.com\/michaelhogannui\/statuses\/687010867108573185","license":"gnip","rt":["RebeccaGelding"],"citation_ids":[4985055],"posted_on":"2016-01-12T20:38:49+00:00","author":{"name":"Michael Hogan","url":"http:\/\/michaelhoganpsychology.com\/","image":"https:\/\/pbs.twimg.com\/profile_images\/904798686114402304\/FoGgRlfv_normal.jpg","description":"Psychology, Neuroscience, Systems Science, Education, Collaboration, Graphicacy, Creativity, Pragmatism, Human Development, Cultural Evolution; Dad :)","id_on_source":"michaelhogannui","tweeter_id":"213405100","geo":{"lt":53,"ln":-8,"country":"IE"},"followers":3261},"tweet_id":"687010867108573185"},{"url":"http:\/\/twitter.com\/psych2evidence\/statuses\/688030551920500736","license":"gnip","citation_ids":[4985055],"posted_on":"2016-01-15T16:10:41+00:00","author":{"name":"Psych2 Evidence","image":"https:\/\/pbs.twimg.com\/profile_images\/590651515787993088\/48DTFkEi_normal.jpg","description":"Real-time Evidence-Based Psychiatry Online #ebm #digital #technology #evidence #psychiatry #mentalhealth #futuremedicine Dr Naik @UniofOxford EBHC Student","id_on_source":"psych2evidence","tweeter_id":"3192717557","geo":{"lt":51.75222,"ln":-1.25596,"country":"GB"},"followers":974},"tweet_id":"688030551920500736"},{"url":"http:\/\/twitter.com\/mripapers\/statuses\/688479978065772544","license":"gnip","citation_ids":[4985055],"posted_on":"2016-01-16T21:56:32+00:00","author":{"name":"MRI Papers","url":"http:\/\/mripapers.tumblr.com","image":"https:\/\/pbs.twimg.com\/profile_images\/646060957777686528\/gbYC6GOS_normal.png","description":"Links to new articles from MRM, NMR in Biomedicine, Neuroimage, JCBFM and HBM. Now available on the web via Tumblr!","id_on_source":"mripapers","tweeter_id":"3728141775","geo":{"lt":null,"ln":null},"followers":701},"tweet_id":"688479978065772544"},{"url":"http:\/\/twitter.com\/AV_SP\/statuses\/688500178614026240","license":"gnip","citation_ids":[4985055],"posted_on":"2016-01-16T23:16:48+00:00","author":{"name":"AV Speech Processing","url":"http:\/\/avisa.loria.fr\/","image":"https:\/\/pbs.twimg.com\/profile_images\/892275071179382784\/VoVEVad4_normal.jpg","description":"The unofficial account of Auditory-VIsual Speech Association. AV speech references + what interests me https:\/\/goo.gl\/oblTmr","id_on_source":"AV_SP","tweeter_id":"2733383114","geo":{"lt":null,"ln":null},"followers":520},"tweet_id":"688500178614026240"}]}}