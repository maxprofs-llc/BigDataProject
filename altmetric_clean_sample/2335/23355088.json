{"altmetric_id":23355088,"counts":{"readers":{"mendeley":6,"citeulike":0,"connotea":0},"total":{"posts_count":9},"twitter":{"unique_users_count":4,"unique_users":["gngdb_rss_bot","arxiv_cscv","arxiv_cs_CV","ComputerPapers"],"posts_count":7},"facebook":{"unique_users_count":2,"unique_users":["411075675932237","175548272963666"],"posts_count":2}},"citation":{"abstract":"With the increased focus on visual attention (VA) in the last decade, a large number of computational visual saliency methods have been developed over the past few years. These models are traditionally evaluated by using performance evaluation metrics that quantify the match between predicted saliency and fixation data obtained from eye-tracking experiments on human observers. Though a considerable number of such metrics have been proposed in the literature, there are notable problems in them. In this paper, we discuss shortcomings in the existing metrics through illustrative examples and propose a new metric that uses local weights based on fixation density, which overcomes these flaws. To compare the performance of our proposed metric at assessing the quality of saliency prediction with other existing metrics, we construct a ground-truth subjective database in which saliency maps obtained from 17 different VA models are evaluated by 16 human observers on a five-point categorical scale in terms of their visual resemblance with corresponding ground-truth fixation density maps obtained from eye-tracking data. The metrics are evaluated by correlating metric scores with the human subjective ratings. The correlation results show that the proposed evaluation metric outperforms all other popular existing metrics. In addition, the constructed database and corresponding subjective ratings provide an insight into which of the existing metrics and future metrics are better at estimating the quality of saliency prediction and can be used as a benchmark.","altmetric_jid":"4f6fa4ed3cf058f610002a5c","arxiv_id":"1708.00169","authors":["Milind S. Gide","Lina J. Karam"],"doi":"10.1109\/tip.2016.2577498","first_seen_on":"2017-08-02T00:32:36+00:00","issns":["1057-7149","1941-0042"],"issue":"8","journal":"IEEE Transactions on Image Processing","last_mentioned_on":1501681272,"links":["https:\/\/arxiv.org\/abs\/1708.00169","https:\/\/arxiv.org\/abs\/1708.00169v1","http:\/\/arxiv.org\/pdf\/1708.00169v1.pdf"],"pdf_url":"http:\/\/arxiv.org\/pdf\/1708.00169v1","pmid":"27295671","pubdate":"2017-08-01T05:39:08+00:00","publisher":"IEEE","publisher_subjects":[{"name":"Artificial Intelligence And Image Processing","scheme":"era"},{"name":"Electrical And Electronic Engineering","scheme":"era"},{"name":"Cognitive Science","scheme":"era"}],"scopus_subjects":["Physical Sciences","Computer Science"],"subjects":["medicalinformatics"],"title":"A Locally Weighted Fixation Density-Based Metric for Assessing the Quality of Visual Saliency Predictions","type":"article","volume":"25","mendeley_url":"http:\/\/www.mendeley.com\/research\/locally-weighted-fixation-densitybased-metric-assessing-quality-visual-saliency-predictions"},"altmetric_score":{"score":1.5,"score_history":{"1y":1.5,"6m":1.5,"3m":1.5,"1m":0,"1w":0,"6d":0,"5d":0,"4d":0,"3d":0,"2d":0,"1d":0,"at":1.5},"context_for_score":{"all":{"total_number_of_other_articles":8314034,"mean":7.0059620328577,"rank":3936745,"this_scored_higher_than_pct":50,"this_scored_higher_than":4192550,"rank_type":"exact","sample_size":8314034,"percentile":50},"similar_age_3m":{"total_number_of_other_articles":174900,"mean":11.743973207394,"rank":79370,"this_scored_higher_than_pct":51,"this_scored_higher_than":90182,"rank_type":"exact","sample_size":174900,"percentile":51},"this_journal":{"total_number_of_other_articles":292,"mean":2.2209690721649,"rank":98,"this_scored_higher_than_pct":65,"this_scored_higher_than":190,"rank_type":"exact","sample_size":292,"percentile":65},"similar_age_this_journal_3m":{"total_number_of_other_articles":13,"mean":1.3125,"rank":3,"this_scored_higher_than_pct":76,"this_scored_higher_than":10,"rank_type":"exact","sample_size":13,"percentile":76}}},"demographics":{"poster_types":{"member_of_the_public":4},"users":{"twitter":{"cohorts":{"Members of the public":4}},"mendeley":{"by_status":{"Librarian":2,"Student  > Ph. D. Student":3,"Other":1},"by_discipline":{"Engineering":1,"Computer Science":5}}}},"posts":{"twitter":[{"url":"http:\/\/twitter.com\/gngdb_rss_bot\/statuses\/892543572423192576","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T00:32:24+00:00","author":{"name":"gngdb rss bot","image":"https:\/\/abs.twimg.com\/sticky\/default_profile_images\/default_profile_normal.png","id_on_source":"gngdb_rss_bot","tweeter_id":"3395908937","geo":{"lt":null,"ln":null},"followers":10},"tweet_id":"892543572423192576"},{"url":"http:\/\/twitter.com\/arxiv_cscv\/statuses\/892545763334553600","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T00:41:07+00:00","author":{"name":"arXiv CS-CV","url":"https:\/\/github.com\/ozan\/arxiv-twitter","image":"https:\/\/pbs.twimg.com\/profile_images\/736660587686416385\/V8FO6CoZ_normal.jpg","description":"All recent computer vision articles from http:\/\/arXiv.org","id_on_source":"arxiv_cscv","tweeter_id":"736627781421826048","geo":{"lt":null,"ln":null},"followers":999},"tweet_id":"892545763334553600"},{"url":"http:\/\/twitter.com\/arxiv_cs_CV\/statuses\/892565765387198464","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T02:00:36+00:00","author":{"name":"cs.CV Papers","image":"https:\/\/abs.twimg.com\/sticky\/default_profile_images\/default_profile_normal.png","description":"unofficial updates on arXiv cs.CV (computer science. computer vision) papers","id_on_source":"arxiv_cs_CV","tweeter_id":"4127701032","geo":{"lt":null,"ln":null},"followers":230},"tweet_id":"892565765387198464"},{"url":"http:\/\/twitter.com\/ComputerPapers\/statuses\/892582643560308736","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T03:07:40+00:00","author":{"name":"Computing Research","url":"http:\/\/arxiv.org\/archive\/cs","image":"https:\/\/pbs.twimg.com\/profile_images\/1149879202\/arxiv_normal.png","description":"New Computing Research submissions to http:\/\/arxiv.org (not affiliated with http:\/\/arxiv.org)","id_on_source":"ComputerPapers","tweeter_id":"135746708","geo":{"lt":null,"ln":null},"followers":79},"tweet_id":"892582643560308736"},{"url":"http:\/\/twitter.com\/arxiv_cscv\/statuses\/892606132182671360","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T04:41:00+00:00","author":{"name":"arXiv CS-CV","url":"https:\/\/github.com\/ozan\/arxiv-twitter","image":"https:\/\/pbs.twimg.com\/profile_images\/736660587686416385\/V8FO6CoZ_normal.jpg","description":"All recent computer vision articles from http:\/\/arXiv.org","id_on_source":"arxiv_cscv","tweeter_id":"736627781421826048","geo":{"lt":null,"ln":null},"followers":999},"tweet_id":"892606132182671360"},{"url":"http:\/\/twitter.com\/arxiv_cscv\/statuses\/892727032995430400","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T12:41:25+00:00","author":{"name":"arXiv CS-CV","url":"https:\/\/github.com\/ozan\/arxiv-twitter","image":"https:\/\/pbs.twimg.com\/profile_images\/736660587686416385\/V8FO6CoZ_normal.jpg","description":"All recent computer vision articles from http:\/\/arXiv.org","id_on_source":"arxiv_cscv","tweeter_id":"736627781421826048","geo":{"lt":null,"ln":null},"followers":999},"tweet_id":"892727032995430400"},{"url":"http:\/\/twitter.com\/arxiv_cscv\/statuses\/892742080367403011","license":"gnip","citation_ids":[23355088],"posted_on":"2017-08-02T13:41:12+00:00","author":{"name":"arXiv CS-CV","url":"https:\/\/github.com\/ozan\/arxiv-twitter","image":"https:\/\/pbs.twimg.com\/profile_images\/736660587686416385\/V8FO6CoZ_normal.jpg","description":"All recent computer vision articles from http:\/\/arXiv.org","id_on_source":"arxiv_cscv","tweeter_id":"736627781421826048","geo":{"lt":null,"ln":null},"followers":999},"tweet_id":"892742080367403011"}],"facebook":[{"title":"Timeline Photos","url":"https:\/\/www.facebook.com\/permalink.php?story_fbid=468607713512366&id=411075675932237","license":"public","citation_ids":[23355088,23355088],"posted_on":"2017-08-02T03:14:47+00:00","summary":"#csCV [\uc2dc\uac01\uc801 \uc778 \ud604\ucd9c \uc131 \uc608\uce21\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\uae30\uc704\ud55c \uad6d\ubd80\uc801\uc73c\ub85c \uac00\uc911\uce58 \uace0\uc815 \ubc00\ub3c4 \uae30\ubc18\uc758 \uce21\uc815 \uae30\uc900] \uc800\uc790 : Lina J. Karam. \uc9c0\ub09c 10 \ub144 \ub3d9\uc548 \uc2dc\uac01\uc801 \uc778\uc8fc\uc758 \uc9d1\uc911 (VA)\uc5d0 \ub300\ud55c \uad00\uc2ec\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc9c0\ub09c \uba87 \ub144 \ub3d9\uc548 \ub9ce\uc740 \uc218\uc758 \uc2dc\uac01\uc801 \ud604\ucd9c \uce21\uc815 \ubc29\ubc95\uc774 \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc740 \uc804\ud1b5\uc801\uc73c\ub85c \uc778\uac04 \uad00\uce21\uc790\uc5d0 \ub300\ud55c \uc548\uad6c \ucd94\uc801 \uc2e4\ud5d8\uc5d0\uc11c \uc5bb\uc740 \uc608\uce21 \ub3cc\ucd9c \ub3c4\uc640 \uace0\uc815 \ub370\uc774\ud130 \uac04\uc758 \uc77c\uce58\ub97c \uc815\ub7c9\ud654\ud558\ub294 \uc131\ub2a5 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub429\ub2c8\ub2e4. \ube44\ub85d","author":{"name":"ArxivSanityKR","url":"https:\/\/www.facebook.com\/411075675932237","facebook_wall_name":"ArxivSanityKR","image":"https:\/\/graph.facebook.com\/411075675932237\/picture","id_on_source":"411075675932237"}},{"title":"Timeline Photos","url":"https:\/\/www.facebook.com\/permalink.php?story_fbid=253727131812446&id=175548272963666","license":"public","citation_ids":[23355088,23355088],"posted_on":"2017-08-02T03:14:39+00:00","summary":"#csCV [A Locally Weighted Fixation Density-Based Metric for Assessing the  Quality of Visual Saliency Predictions]\n\nAuthors: , Lina J. Karam.\n\nWith the increased focus on visual attention (VA) in the last decade, a large number of computational visual sali","author":{"name":"Arxiv Sanity","url":"https:\/\/www.facebook.com\/175548272963666","facebook_wall_name":"Arxiv Sanity","image":"https:\/\/graph.facebook.com\/175548272963666\/picture","id_on_source":"175548272963666"}}]}}